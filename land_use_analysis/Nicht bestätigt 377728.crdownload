{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KraKA0RlLuDJ"
      },
      "source": [
        "### Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhfNg0IsLuDL"
      },
      "outputs": [],
      "source": [
        "# Install libraries\n",
        "!pip install geopandas\n",
        "!pip install rasterio\n",
        "\n",
        "# Load libraries\n",
        "%matplotlib inline\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import pylab as py\n",
        "import statistics\n",
        "from scipy.stats import yeojohnson, pearsonr\n",
        "from scipy.cluster import hierarchy\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, cophenet\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.cross_decomposition import CCA\n",
        "\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__>= \"2.0\"\n",
        "\n",
        "import sys\n",
        "assert sys.version_info >= (3,5)\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ql4tjOn-LuDM"
      },
      "outputs": [],
      "source": [
        "# Ignore useless warnings (see SciPy issue #5998)\n",
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
        "\n",
        "# Get all the rows as output\n",
        "pd.set_option('display.max_columns', 190)\n",
        "pd.set_option('display.max_rows', 160)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asMeNotTLuDN"
      },
      "outputs": [],
      "source": [
        "conda install -c conda-forge rasterio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeaSJlyLLuDN"
      },
      "source": [
        "### Change and Transition Matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1PbXY2pLuDN",
        "outputId": "dc3a270d-70e0-4a6a-fef2-2b5ba202641b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "change matrix:\n",
            "          0       1        2        3        4      5        6      7\n",
            "0  16340475       0        0        0        0      0        0      0\n",
            "1         0  264134       24      443       61    158        0      6\n",
            "2         0    1494  2099063      382     4621    652     2877     45\n",
            "3         0    9992    14524  8616351     1477    129       26  20966\n",
            "4         0     229      742      368  1581739    154      165     63\n",
            "5         0       0        0        0        0  18206        0      0\n",
            "6         0     290     3267        0     1337    279  1051882      0\n",
            "7         0       2       17      202        2     31        0  64735\n",
            "transition matrix:\n",
            "     0         1         2         3         4         5         6         7\n",
            "0  1.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
            "1  0.0  0.997387  0.000091  0.001673  0.000230  0.000597  0.000000  0.000023\n",
            "2  0.0  0.000708  0.995225  0.000181  0.002191  0.000309  0.001364  0.000021\n",
            "3  0.0  0.001153  0.001676  0.994562  0.000170  0.000015  0.000003  0.002420\n",
            "4  0.0  0.000145  0.000469  0.000232  0.998913  0.000097  0.000104  0.000040\n",
            "5  0.0  0.000000  0.000000  0.000000  0.000000  1.000000  0.000000  0.000000\n",
            "6  0.0  0.000274  0.003091  0.000000  0.001265  0.000264  0.995106  0.000000\n",
            "7  0.0  0.000031  0.000262  0.003108  0.000031  0.000477  0.000000  0.996092\n"
          ]
        }
      ],
      "source": [
        "import rasterio\n",
        "\n",
        "# FUNCTION: read .tif\n",
        "def read_raster(file_path):\n",
        "    with rasterio.open(file_path) as src:\n",
        "        return src.read(1), src.transform\n",
        "\n",
        "# FUNCTION: calculate change matrix\n",
        "def calculate_change_matrix(before, after):\n",
        "    unique_before = np.unique(before)\n",
        "    unique_after = np.unique(after)\n",
        "    all_classes = np.unique(np.concatenate((unique_before, unique_after)))\n",
        "\n",
        "    change_matrix = pd.DataFrame(0, index=all_classes, columns=all_classes)\n",
        "\n",
        "    for i in range(before.shape[0]):\n",
        "        for j in range(before.shape[1]):\n",
        "            from_class = before[i, j]\n",
        "            to_class = after[i, j]\n",
        "            change_matrix.loc[from_class, to_class] += 1\n",
        "\n",
        "    return change_matrix\n",
        "\n",
        "# FUNCTION: calculate transition matrix\n",
        "def calculate_transition_probabilities(change_matrix):\n",
        "    transition_matrix = change_matrix.div(change_matrix.sum(axis=1), axis=0)\n",
        "    return transition_matrix\n",
        "\n",
        "# FUNCTION: make it round\n",
        "def main(before_file, after_file):\n",
        "    # read input\n",
        "    before, _ = read_raster(before_file)\n",
        "    after, _ = read_raster(after_file)\n",
        "\n",
        "    # change matrix\n",
        "    change_matrix = calculate_change_matrix(before, after)\n",
        "    print(\"change matrix:\")\n",
        "    print(change_matrix)\n",
        "\n",
        "    # transition matrix\n",
        "    transition_matrix = calculate_transition_probabilities(change_matrix)\n",
        "    print(\"transition matrix:\")\n",
        "    print(transition_matrix)\n",
        "\n",
        "    return change_matrix, transition_matrix\n",
        "\n",
        "before_file = 'D:\\\\AML\\\\datasets\\\\rc_1992.tif'\n",
        "after_file = 'D:\\\\AML\\\\datasets\\\\rc_1996.tif'\n",
        "change_matrix, transition_matrix = main(before_file, after_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9yauyVSLuDO"
      },
      "source": [
        "### Transformation/Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwgaErpKLuDO"
      },
      "outputs": [],
      "source": [
        "from rasterio.transform import from_origin\n",
        "\n",
        "current_land_use_map = 'D:\\\\AML\\\\datasets\\\\rc_1997.tif'\n",
        "data_directory = \"D:\\\\AML\\\\output\\\\validation\"\n",
        "\n",
        "# current land use map\n",
        "with rasterio.open(current_land_use_map) as src:\n",
        "    current_land_use_map = src.read(1)\n",
        "    transform = src.transform  # spatial transform\n",
        "\n",
        "# define transition probs\n",
        "transition_probs = transition_matrix\n",
        "\n",
        "# probability validity (between 0 and 1 and sum to 1 for each row)\n",
        "if (transition_probs < 0).any() or (transition_probs > 1).any() or not np.allclose(transition_probs.sum(axis=1), 1):\n",
        "    raise ValueError(\"Invalid transition probabilities.\")\n",
        "\n",
        "# FUNCTION: apply the transition by transition probs\n",
        "def apply_transition(land_use, transition_probs):\n",
        "    new_land_use = np.copy(land_use)\n",
        "    rows, cols = land_use.shape\n",
        "    for row in range(rows):\n",
        "        for col in range(cols):\n",
        "            current_category = int(land_use[row, col])\n",
        "            if 1 <= current_category <= 5:\n",
        "                transition_probs_normalized = transition_probs[current_category - 1]\n",
        "                new_category = np.argmax(np.random.multinomial(1, transition_probs_normalized))\n",
        "                new_land_use[row, col] = new_category + 1\n",
        "    return new_land_use\n",
        "\n",
        "# apply transition to current land use map\n",
        "predicted_land_use = apply_transition(current_land_use_map, transition_probs)\n",
        "\n",
        "# shape of predicted land use map for saving\n",
        "rows, cols = predicted_land_use.shape\n",
        "\n",
        "# save predicted land use map as .tif\n",
        "output_raster_path = os.path.join(data_directory, 'predicted_land_use_1997.tif')\n",
        "with rasterio.open(output_raster_path, 'w', driver='GTiff', width=cols, height=rows, count=1, dtype=rasterio.int32, crs=src.crs, transform=transform) as dst:\n",
        "    dst.write(predicted_land_use, 1)\n",
        "\n",
        "print(\"Prediction completed. The result is saved to:\", output_raster_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validierung"
      ],
      "metadata": {
        "id": "QTz-wXDsqNEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error, cohen_kappa_score, confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "class Validation:\n",
        "    \"\"\"\n",
        "    A class to validate spatial prediction data against ground truth data.\n",
        "\n",
        "    Attributes:\n",
        "    ----------\n",
        "    truth_path : str\n",
        "        The file path to the ground truth data.\n",
        "    predicted_path : str\n",
        "        The file path to the predicted data.\n",
        "    truth_label_col : str, optional\n",
        "        The column name of the ground truth labels (default is 'grid_code').\n",
        "    predicted_label_col : str, optional\n",
        "        The column name of the predicted labels (default is 'grid_code').\n",
        "    truth_data : GeoDataFrame\n",
        "        The loaded ground truth data.\n",
        "    predicted_data : GeoDataFrame\n",
        "        The loaded predicted data.\n",
        "    merged_data : DataFrame\n",
        "        The merged data containing both ground truth and predicted labels.\n",
        "    metrics : dict\n",
        "        A dictionary to store calculated metrics.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, truth_path: str, predicted_path: str, truth_label_col: str = 'grid_code', predicted_label_col: str = 'grid_code'):\n",
        "        self.truth_path = truth_path\n",
        "        self.predicted_path = predicted_path\n",
        "        self.truth_label_col = truth_label_col\n",
        "        self.predicted_label_col = predicted_label_col\n",
        "        self.truth_data = None\n",
        "        self.predicted_data = None\n",
        "        self.merged_data = None\n",
        "        self.metrics = {}\n",
        "\n",
        "    def load_data(self) -> None:\n",
        "        \"\"\"Loads the ground truth and predicted data from the given file paths.\"\"\"\n",
        "        try:\n",
        "            logging.info(\"Loading ground truth data from %s\", self.truth_path)\n",
        "            self.truth_data = gpd.read_file(self.truth_path)\n",
        "            logging.info(\"Loading predicted data from %s\", self.predicted_path)\n",
        "            self.predicted_data = gpd.read_file(self.predicted_path)\n",
        "        except Exception as e:\n",
        "            logging.error(\"Error loading data: %s\", e)\n",
        "            raise ValueError(f\"Error loading data: {e}\")\n",
        "\n",
        "    def preprocess_data(self) -> None:\n",
        "        \"\"\"Preprocesses the data by rounding coordinates and creating unique identifiers.\"\"\"\n",
        "        logging.info(\"Preprocessing data\")\n",
        "        self.truth_data['rounded_x'] = self.truth_data.geometry.x.round(6)\n",
        "        self.truth_data['rounded_y'] = self.truth_data.geometry.y.round(6)\n",
        "        self.predicted_data['rounded_x'] = self.predicted_data.geometry.x.round(6)\n",
        "        self.predicted_data['rounded_y'] = self.predicted_data.geometry.y.round(6)\n",
        "\n",
        "        self.truth_data['unique_id'] = self.truth_data.geometry.apply(lambda geom: f'{geom.x:.6f}_{geom.y:.6f}')\n",
        "        self.predicted_data['unique_id'] = self.predicted_data.geometry.apply(lambda geom: f'{geom.x:.6f}_{geom.y:.6f}')\n",
        "\n",
        "    def merge_data(self) -> None:\n",
        "        \"\"\"Merges the ground truth and predicted data on unique identifiers.\"\"\"\n",
        "        logging.info(\"Merging ground truth and predicted data\")\n",
        "        self.merged_data = self.truth_data.merge(self.predicted_data, on='unique_id', how='inner')\n",
        "\n",
        "        if len(self.truth_data) != len(self.predicted_data):\n",
        "            logging.warning(\"The number of points in truth and predicted datasets is not the same.\")\n",
        "\n",
        "        if len(self.merged_data) == 0:\n",
        "            logging.error(\"There are no common points between the truth and predicted datasets.\")\n",
        "            raise ValueError(\"Error: There are no common points between the truth and predicted datasets.\")\n",
        "\n",
        "    def plot_data(self) -> None:\n",
        "        \"\"\"Plots the ground truth and predicted data on a map.\"\"\"\n",
        "        logging.info(\"Plotting data\")\n",
        "        ax = self.truth_data.plot(color='blue', label='Truth Data')\n",
        "        self.predicted_data.plot(ax=ax, color='red', label='Predicted Data')\n",
        "        plt.legend()\n",
        "        plt.title('Truth vs Predicted Data')\n",
        "        plt.xlabel('Longitude')\n",
        "        plt.ylabel('Latitude')\n",
        "        plt.show()\n",
        "\n",
        "    def calculate_metrics(self) -> None:\n",
        "        \"\"\"Calculates various validation metrics comparing ground truth and predicted labels.\"\"\"\n",
        "        logging.info(\"Calculating metrics\")\n",
        "        truth_labels = self.merged_data[f'{self.truth_label_col}_x'].astype(int)\n",
        "        predicted_labels = self.merged_data[f'{self.predicted_label_col}_y'].astype(int)\n",
        "\n",
        "        self.metrics['Accuracy'] = accuracy_score(truth_labels, predicted_labels)\n",
        "        self.metrics['MAE'] = mean_absolute_error(truth_labels, predicted_labels)\n",
        "        self.metrics['RMSE'] = mean_squared_error(truth_labels, predicted_labels, squared=False)\n",
        "        self.metrics['Cohen\\'s Kappa'] = cohen_kappa_score(truth_labels, predicted_labels)\n",
        "        self.metrics['Confusion Matrix'] = confusion_matrix(truth_labels, predicted_labels)\n",
        "        self.metrics['Classification Report'] = classification_report(truth_labels, predicted_labels, output_dict=True)\n",
        "\n",
        "    def display_metrics(self) -> None:\n",
        "        \"\"\"Displays the calculated validation metrics.\"\"\"\n",
        "        logging.info(\"Displaying metrics\")\n",
        "        df_metrics = pd.DataFrame(self.metrics['Classification Report']).transpose()\n",
        "        df_metrics['Accuracy'] = self.metrics['Accuracy']\n",
        "        df_metrics['MAE'] = self.metrics['MAE']\n",
        "        df_metrics['RMSE'] = self.metrics['RMSE']\n",
        "        df_metrics['Cohen\\'s Kappa'] = self.metrics['Cohen\\'s Kappa']\n",
        "        df_confusion = pd.DataFrame(self.metrics['Confusion Matrix'])\n",
        "\n",
        "        print(\"\\nAccuracy, MAE, RMSE, Cohen's Kappa:\\n\", df_metrics)\n",
        "        print(\"\\nConfusion Matrix:\\n\", df_confusion)\n",
        "\n",
        "    def validate(self) -> None:\n",
        "        \"\"\"Runs the complete validation process: loading data, preprocessing, merging, plotting, and calculating metrics.\"\"\"\n",
        "        logging.info(\"Starting validation process\")\n",
        "        self.load_data()\n",
        "        self.preprocess_data()\n",
        "        self.merge_data()\n",
        "        self.plot_data()\n",
        "        self.calculate_metrics()\n",
        "        self.display_metrics()\n",
        "        logging.info(\"Validation process completed\")\n",
        "\n",
        "# Example\n",
        "truth_path = \"/content/drive/MyDrive/AML_group_project/AML_project/datasets/final_project/validation/validation_cut/export_original_1997_PairwiseClip.shp\"\n",
        "predicted_path = \"/content/drive/MyDrive/AML_group_project/AML_project/datasets/final_project/validation/validation_cut/export_predicted_1997_PairwiseClip.shp\"\n",
        "validator = Validation(truth_path, predicted_path)\n",
        "validator.validate()\n"
      ],
      "metadata": {
        "id": "eO3nTstPqQ3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualisation of Land use Change"
      ],
      "metadata": {
        "id": "6njhYCquqjHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import logging\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import rasterio\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Optional, Union\n",
        "from rasterio.plot import show\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "class LandUseMapPlotter:\n",
        "    def __init__(self, file_paths: List[str], years: List[int], land_use_colors: Dict[int, str], land_use_labels: Dict[int, str], gridcode_column: str = 'gridcode'):\n",
        "        \"\"\"\n",
        "        Initialize the LandUseMapPlotter.\n",
        "\n",
        "        Parameters:\n",
        "        - file_paths (List[str]): List of file paths for shapefiles and raster files.\n",
        "        - years (List[int]): List of years corresponding to the file paths.\n",
        "        - land_use_colors (Dict[int, str]): Dictionary mapping land use codes to colors.\n",
        "        - land_use_labels (Dict[int, str]): Dictionary mapping land use codes to labels.\n",
        "        - gridcode_column (str): Column name for land use codes. Default is 'gridcode'.\n",
        "\n",
        "        Example:\n",
        "            file_paths = ['data/land_use_1990.shp', 'data/land_use_2000.shp']\n",
        "            years = [1990, 2000]\n",
        "            land_use_colors = {1: \"yellow\", 2: \"green\"}\n",
        "            land_use_labels = {1: \"Agriculture\", 2: \"Forest\"}\n",
        "\n",
        "            plotter = LandUseMapPlotter(file_paths, years, land_use_colors, land_use_labels, gridcode_column='land_code')\n",
        "        \"\"\"\n",
        "        if len(file_paths) != len(years):\n",
        "            raise ValueError(f\"Lengths of file_paths (length: {len(file_paths)}) and years (length: {len(years)}) do not match\")\n",
        "        if len(land_use_colors) != len(land_use_labels):\n",
        "            raise ValueError(\"Lengths of land_use_colors and land_use_labels do not match\")\n",
        "\n",
        "        self.file_paths = file_paths\n",
        "        self.years = years\n",
        "        self.land_use_colors = land_use_colors\n",
        "        self.land_use_labels = land_use_labels\n",
        "        self.gridcode_column = gridcode_column\n",
        "        self.geodataframes = []\n",
        "        self.raster_datasets = []\n",
        "        self.land_use_counts = pd.DataFrame(columns=[land_use_labels[i] for i in land_use_labels.keys()])\n",
        "\n",
        "\n",
        "    def load_files(self) -> None:\n",
        "        \"\"\"\n",
        "        Load shapefiles and raster files based on the provided paths.\n",
        "\n",
        "        Example:\n",
        "            plotter.load_files()\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "        for path in self.file_paths:\n",
        "            try:\n",
        "                if path.endswith('.tif'):\n",
        "                    self._load_raster_file(path)\n",
        "                elif path.endswith('.shp'):\n",
        "                    self._load_shapefile(path, start_time)\n",
        "                else:\n",
        "                    logger.warning(f\"Unsupported file format for '{path}'\")\n",
        "                    self._append_none()\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error loading file '{path}': {e}\")\n",
        "                self._append_none()\n",
        "\n",
        "        logger.info(f\"Total loaded geodataframes: {len([gdf for gdf in self.geodataframes if gdf is not None])}\")\n",
        "\n",
        "    def _load_raster_file(self, path: str) -> None:\n",
        "        \"\"\"Load a raster file.\"\"\"\n",
        "        try:\n",
        "            self.raster_datasets.append(rasterio.open(path))\n",
        "            self.geodataframes.append(None)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load raster file '{path}': {e}\")\n",
        "            self._append_none()\n",
        "\n",
        "    def _load_shapefile(self, path: str, start_time: float) -> None:\n",
        "        \"\"\"Load a shapefile.\"\"\"\n",
        "        try:\n",
        "            load_time = time.time() - start_time\n",
        "            gdf = gpd.read_file(path)\n",
        "            if self.gridcode_column not in gdf.columns:\n",
        "                raise ValueError(f\"Column '{self.gridcode_column}' not found in shapefile '{path}'. Please rename the column to '{self.gridcode_column}' in your GIS system.\")\n",
        "            self.geodataframes.append(gdf)\n",
        "            self.raster_datasets.append(None)\n",
        "            logger.info(f\"Loaded shapefile '{path}' with {len(gdf)} records in {load_time:.2f} seconds.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load shapefile '{path}': {e}\")\n",
        "            self._append_none()\n",
        "\n",
        "    def _append_none(self) -> None:\n",
        "        \"\"\"Append None to both geodataframes and raster datasets lists.\"\"\"\n",
        "        self.raster_datasets.append(None)\n",
        "        self.geodataframes.append(None)\n",
        "\n",
        "    def plot_maps(self, figsize: tuple = (15, 10), num_cols: int = 3, save_path: Optional[str] = None, save_format: str = 'png', show_plot: bool = True) -> None:\n",
        "        \"\"\"\n",
        "        Plot land use maps for each year.\n",
        "\n",
        "        Parameters:\n",
        "        - figsize (tuple): Figure size for the plot.\n",
        "        - num_cols (int): Number of columns in the subplot grid.\n",
        "        - save_path (Optional[str]): Path to save the plot.\n",
        "        - save_format (str): Format to save the plot ('png', 'jpg', 'pdf', etc.).\n",
        "        - show_plot (bool): Whether to display the plot.\n",
        "\n",
        "        Example:\n",
        "            plotter.plot_maps(save_path='plots/land_use_maps.png', save_format='png', show_plot=False)\n",
        "        \"\"\"\n",
        "        num_files = len(self.file_paths)\n",
        "        num_rows = math.ceil(num_files / num_cols)\n",
        "\n",
        "        fig, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        for i, (gdf, dataset, year, path) in enumerate(zip(self.geodataframes, self.raster_datasets, self.years, self.file_paths)):\n",
        "            ax = axes[i]\n",
        "            title_suffix = \"Original\" if \"original\" in path else \"Predicted\"\n",
        "            if gdf is not None:\n",
        "                self.plot_single_map(ax, gdf, year, title_suffix)\n",
        "            elif dataset is not None:\n",
        "                self.plot_single_raster(ax, dataset, year, title_suffix)\n",
        "\n",
        "        if num_files < len(axes):\n",
        "            self.add_legend(axes[num_files])\n",
        "            for j in range(num_files + 1, len(axes)):\n",
        "                fig.delaxes(axes[j])\n",
        "        else:\n",
        "            self.add_legend(axes[-1])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.subplots_adjust(wspace=0, hspace=0)\n",
        "\n",
        "        if save_path:\n",
        "            try:\n",
        "                if not os.path.exists(os.path.dirname(save_path)):\n",
        "                    os.makedirs(os.path.dirname(save_path))\n",
        "                    logger.info(f\"Created directory: {os.path.dirname(save_path)}\")\n",
        "\n",
        "                plt.savefig(save_path, format=save_format)\n",
        "                logger.info(f\"Plot saved successfully to {save_path}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Failed to save plot to {save_path}: {e}\")\n",
        "\n",
        "        if show_plot:\n",
        "            plt.show()\n",
        "        else:\n",
        "            plt.close(fig)\n",
        "\n",
        "    def plot_single_map(self, ax: plt.Axes, gdf: gpd.GeoDataFrame, year: int, title_suffix: str) -> None:\n",
        "        \"\"\"\n",
        "        Plot a single shapefile map.\n",
        "\n",
        "        Parameters:\n",
        "        - ax (plt.Axes): Matplotlib Axes object.\n",
        "        - gdf (gpd.GeoDataFrame): GeoDataFrame containing the shapefile data.\n",
        "        - year (int): Year corresponding to the map.\n",
        "        - title_suffix (str): Suffix to add to the title to distinguish original and predicted maps.\n",
        "\n",
        "        Example:\n",
        "            plotter.plot_single_map(ax, gdf, 1990, \"Original\")\n",
        "        \"\"\"\n",
        "        for land_use_code, color in self.land_use_colors.items():\n",
        "            gdf[gdf[self.gridcode_column] == land_use_code].plot(ax=ax, color=color)\n",
        "        ax.set_title(f'Year {year} ({title_suffix})', fontsize=16, fontweight='bold')\n",
        "        ax.axis('off')\n",
        "\n",
        "    def plot_single_raster(self, ax: plt.Axes, dataset: rasterio.io.DatasetReader, year: int, title_suffix: str) -> None:\n",
        "        \"\"\"\n",
        "        Plot a single raster map.\n",
        "\n",
        "        Parameters:\n",
        "        - ax (plt.Axes): Matplotlib Axes object.\n",
        "        - dataset (rasterio.io.DatasetReader): Raster dataset.\n",
        "        - year (int): Year corresponding to the map.\n",
        "        - title_suffix (str): Suffix to add to the title to distinguish original and predicted maps.\n",
        "\n",
        "        Example:\n",
        "            plotter.plot_single_raster(ax, dataset, 2000, \"Predicted\")\n",
        "        \"\"\"\n",
        "        show((dataset, 1), ax=ax, cmap='viridis', title=f'Year {year} ({title_suffix})')\n",
        "        ax.axis('off')\n",
        "\n",
        "    def add_legend(self, ax: plt.Axes) -> None:\n",
        "        \"\"\"\n",
        "        Add a legend to the plot.\n",
        "\n",
        "        Parameters:\n",
        "        - ax (plt.Axes): Matplotlib Axes object.\n",
        "\n",
        "        Example:\n",
        "            plotter.add_legend(ax)\n",
        "        \"\"\"\n",
        "        legend_patches = []\n",
        "        for land_use_code, color in self.land_use_colors.items():\n",
        "            legend_label = self.land_use_labels.get(land_use_code, f'Land Use {land_use_code}')\n",
        "            patch = plt.Rectangle((0, 0), 1, 1, facecolor=color, edgecolor='black')\n",
        "            legend_patches.append((patch, legend_label))\n",
        "\n",
        "        ax.legend(*zip(*legend_patches), loc='center', fontsize=12)\n",
        "        ax.axis('off')\n",
        "\n",
        "    def count_land_use(self) -> None:\n",
        "        \"\"\"\n",
        "        Count the number of occurrences for each land use category in shapefiles.\n",
        "\n",
        "        Example:\n",
        "            plotter.count_land_use()\n",
        "        \"\"\"\n",
        "        counts_list = []\n",
        "        for gdf, year in zip(self.geodataframes, self.years):\n",
        "            if gdf is not None:\n",
        "                if self.gridcode_column not in gdf.columns:\n",
        "                    logger.warning(f\"Column '{self.gridcode_column}' not found in data for year {year}. Skipping...\")\n",
        "                    continue\n",
        "                category_counts = {self.land_use_labels[i]: (gdf[self.gridcode_column] == i).sum() for i in self.land_use_labels.keys()}\n",
        "                counts_list.append(category_counts)\n",
        "                logger.info(f\"Year {year} counts: {category_counts}\")\n",
        "\n",
        "        if counts_list:\n",
        "            counts_df = pd.DataFrame(counts_list)\n",
        "            counts_df.index = self.years[:len(counts_list)]\n",
        "            self.land_use_counts = pd.concat([self.land_use_counts, counts_df], axis=0).drop_duplicates()\n",
        "            logger.info(\"Updated land use counts DataFrame:\\n%s\", self.land_use_counts)\n",
        "        else:\n",
        "            logger.info(\"No data to append to land_use_counts DataFrame.\")\n",
        "\n",
        "        def plot_land_use_evolution(self, save_path: Optional[str] = None, save_format: str = 'png', show_plot: bool = True) -> None:\n",
        "            \"\"\"\n",
        "            Plot the evolution of land use categories over time.\n",
        "\n",
        "            Parameters:\n",
        "            - save_path (Optional[str]): Path to save the plot.\n",
        "            - save_format (str): Format to save the plot ('png', 'jpg', 'pdf', etc.).\n",
        "            - show_plot (bool): Whether to display the plot.\n",
        "\n",
        "            Example:\n",
        "                plotter.plot_land_use_evolution(save_path='plots/land_use_evolution.png', save_format='png', show_plot=False)\n",
        "            \"\"\"\n",
        "            self.count_land_use()  # Call count_land_use to ensure land_use_counts is updated\n",
        "\n",
        "            if self.land_use_counts.empty:\n",
        "                logger.warning(\"The land use counts DataFrame is empty. No data to plot.\")\n",
        "                print(\"The land use counts DataFrame is empty. No data to plot.\")  # Print a warning message\n",
        "                return\n",
        "\n",
        "            label_to_code = {label: code for code, label in self.land_use_labels.items()}\n",
        "\n",
        "            print(\"land_use_counts DataFrame:\\n\", self.land_use_counts)  # Print the DataFrame for verification\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            try:\n",
        "                colors = [self.land_use_colors[label_to_code[col]] for col in self.land_use_counts.columns]\n",
        "            except KeyError as e:\n",
        "                logger.error(f\"KeyError: {e}. Check the keys in land_use_colors and columns in land_use_counts DataFrame for mismatches.\")\n",
        "                print(f\"KeyError: {e}. Check the keys in land_use_colors and columns in land_use_counts DataFrame for mismatches.\")  # Print error message\n",
        "                return\n",
        "\n",
        "            self.land_use_counts.plot(kind='bar', stacked=True, color=colors)\n",
        "            plt.title(\"Land Use Evolution Over Time\")\n",
        "            plt.xlabel(\"Year\")\n",
        "            plt.ylabel(\"Land Use Area (Number of Pixels)\")\n",
        "            plt.xticks(range(len(self.years)), self.years, rotation=45)\n",
        "            plt.tight_layout()\n",
        "\n",
        "            if save_path:\n",
        "                try:\n",
        "                    if not os.path.exists(os.path.dirname(save_path)):\n",
        "                        os.makedirs(os.path.dirname(save_path))\n",
        "                        logger.info(f\"Created directory: {os.path.dirname(save_path)}\")\n",
        "\n",
        "                    plt.savefig(save_path, format=save_format)\n",
        "                    logger.info(f\"Plot saved successfully to {save_path}\")\n",
        "                    print(f\"Plot saved successfully to {save_path}\")  # Print success message\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Failed to save plot to {save_path}: {e}\")\n",
        "                    print(f\"Failed to save plot to {save_path}: {e}\")  # Print error message\n",
        "\n",
        "            if show_plot:\n",
        "                plt.show()\n",
        "            else:\n",
        "                plt.close()\n",
        "\n",
        "\n",
        "# Usage\n",
        "RAW_DIRECTORY = \"/content/drive/MyDrive/AML_group_project/AML_project/datasets/final_project/polygon\"\n",
        "file_paths_original = [os.path.join(RAW_DIRECTORY, f) for f in os.listdir(RAW_DIRECTORY) if f.endswith('.shp') and f.startswith('original')]\n",
        "file_paths_predicted = [os.path.join(RAW_DIRECTORY, f) for f in os.listdir(RAW_DIRECTORY) if f.endswith('.shp') and f.startswith('predicted')]\n",
        "\n",
        "# Extract years from the file names\n",
        "def extract_years(file_paths: List[str]) -> List[int]:\n",
        "    \"\"\"Extract years from file names assuming they are in the format 'prefix_year.ext'.\"\"\"\n",
        "    years = []\n",
        "    for file_path in file_paths:\n",
        "        try:\n",
        "            parts = os.path.basename(file_path).split('_')\n",
        "            year = int(parts[1].split('.')[0])\n",
        "            years.append(year)\n",
        "        except (IndexError, ValueError):\n",
        "            logger.warning(f\"Could not extract year from '{file_path}'\")\n",
        "    return years\n",
        "\n",
        "years_original = extract_years(file_paths_original)\n",
        "years_predicted = extract_years(file_paths_predicted)\n",
        "\n",
        "land_use_colors = {\n",
        "    1: \"yellow\",\n",
        "    2: \"limegreen\",\n",
        "    3: \"forestgreen\",\n",
        "    4: \"springgreen\",\n",
        "    5: \"red\",\n",
        "    6: \"lightgrey\",\n",
        "    7: \"blue\"\n",
        "}\n",
        "\n",
        "land_use_labels = {\n",
        "    1: \"Cropland\",\n",
        "    2: \"Natural land cover\",\n",
        "    3: \"Forest\",\n",
        "    4: \"Grassland\",\n",
        "    5: \"Urban Area\",\n",
        "    6: \"Bare Area\",\n",
        "    7: \"Water\"\n",
        "}"
      ],
      "metadata": {
        "id": "BNtOGahEptXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1997\n",
        "\n",
        "lulc_peru_1997 = [\"/content/drive/MyDrive/AML_group_project/AML_project/datasets/final_project/polygon/original_1997.shp\", \"/content/drive/MyDrive/AML_group_project/AML_project/datasets/final_project/polygon/predicted_1997.shp\"]\n",
        "year = [1997, 1997]\n",
        "# Make instance of LandUseMapPlotter and load_files_original\n",
        "plotter_1997 = LandUseMapPlotter(lulc_peru_1997, year, land_use_colors, land_use_labels)\n",
        "plotter_1997.load_files()\n",
        "\n",
        "# Plot maps\n",
        "\n",
        "plotter_1997.plot_maps()"
      ],
      "metadata": {
        "id": "6nSiwJcmq_tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make instance of LandUseMapPlotter and load_files_original\n",
        "plotter_original = LandUseMapPlotter(file_paths_original, years_original, land_use_colors, land_use_labels)\n",
        "plotter_original.load_files()"
      ],
      "metadata": {
        "id": "XWCRVgldrDOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot land use evolution over time\n",
        "plotter_original.plot_land_use_evolution(save_path='/content/drive/MyDrive/AML_group_project/AML_project/datasets/final_project/polygon/plotsland_use_evolution.png', save_format='png', show_plot=True)"
      ],
      "metadata": {
        "id": "k-rs6XFYrHab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot original maps\n",
        "plotter_original.plot_maps()"
      ],
      "metadata": {
        "id": "KFCLp9EvrQ0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualisation of Validation\n",
        "\n",
        "As the authors only had Textfiles as the outcome of the validation-process, they generated another class and function to derive the important statistics from the file. Only the Accuracy, MAE, RMSE and Kohen's Kappa were transfered by hand.\n",
        "\n",
        "The Confusion Matrices, the precision, recall, f1-Score and support were automatically retrived and processed."
      ],
      "metadata": {
        "id": "rXvt77vmrh_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "def process_and_plot_confusion_matrices(folder_path, save_folder, as_percentage=False):\n",
        "    \"\"\"\n",
        "    Process and plot confusion matrices from all .txt files in the specified folder,\n",
        "    and generate a clustered or stacked bar plot of the sum of false negative values for each class across all files.\n",
        "\n",
        "    Parameters:\n",
        "        folder_path (str): The path to the folder containing .txt files with confusion matrices.\n",
        "        save_folder (str): The path to the folder where the plots will be saved.\n",
        "        as_percentage (bool): Whether to display the false negatives as a percentage in a stacked plot.\n",
        "    \"\"\"\n",
        "\n",
        "    # Function to read and parse confusion matrix from a .txt file\n",
        "    def read_confusion_matrix(file_path):\n",
        "        with open(file_path, 'r') as file:\n",
        "            lines = file.readlines()\n",
        "\n",
        "        matrix_start_idx = None\n",
        "        for idx, line in enumerate(lines):\n",
        "            if line.startswith(\"Confusion Matrix:\"):\n",
        "                matrix_start_idx = idx + 1\n",
        "                break\n",
        "\n",
        "        matrix = []\n",
        "        for line in lines[matrix_start_idx + 1:]:\n",
        "            parts = line.strip().split()[1:]  # Skip the row label\n",
        "            if all(part.isdigit() for part in parts):  # Ensure all parts are digits\n",
        "                matrix.append(list(map(int, parts)))\n",
        "\n",
        "        return np.array(matrix)\n",
        "\n",
        "    # Function to plot confusion matrix\n",
        "    def plot_confusion_matrix(matrix, labels, title, save_path):\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.title(title)\n",
        "        plt.savefig(save_path)  # Save the plot\n",
        "        plt.close()  # Close the plot to avoid display in the notebook\n",
        "\n",
        "    # Function to calculate false negatives\n",
        "    def calculate_false_negatives(matrix):\n",
        "        false_negatives = matrix.sum(axis=1) - np.diag(matrix)\n",
        "        return false_negatives\n",
        "\n",
        "    # Create the save folder if it doesn't exist\n",
        "    os.makedirs(save_folder, exist_ok=True)\n",
        "\n",
        "    # Get list of all .txt files in the folder and sort by year\n",
        "    txt_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.txt')], key=lambda x: int(x.split('.')[0]))\n",
        "\n",
        "    # Define the labels and colors\n",
        "    labels = ['1', '2', '3', '4', '5', '6', '7']\n",
        "    land_use_colors = {\n",
        "        '1': \"yellow\",\n",
        "        '2': \"limegreen\",\n",
        "        '3': \"forestgreen\",\n",
        "        '4': \"springgreen\",\n",
        "        '5': \"red\",\n",
        "        '6': \"lightgrey\",\n",
        "        '7': \"blue\"\n",
        "    }\n",
        "\n",
        "    # Initialize dictionary to store false negatives for each class across all files\n",
        "    false_negatives_dict = {label: [] for label in labels}\n",
        "    years = []\n",
        "\n",
        "    # Process each file\n",
        "    for txt_file in txt_files:\n",
        "        file_path = os.path.join(folder_path, txt_file)\n",
        "        confusion_matrix = read_confusion_matrix(file_path)\n",
        "        if confusion_matrix.size > 0:  # Only plot if the matrix is not empty\n",
        "            save_path = os.path.join(save_folder, f'{txt_file[:-4]}_confusion_matrix.png')\n",
        "            plot_confusion_matrix(confusion_matrix, labels, title=f'Confusion Matrix: {txt_file[:-4]}', save_path=save_path)\n",
        "\n",
        "            # Calculate and store false negatives\n",
        "            false_negatives = calculate_false_negatives(confusion_matrix)\n",
        "            for i, label in enumerate(labels):\n",
        "                false_negatives_dict[label].append(false_negatives[i])\n",
        "            years.append(txt_file[:-4])\n",
        "\n",
        "    if as_percentage:\n",
        "        # Convert false negatives to percentages\n",
        "        total_false_negatives_per_year = np.sum(list(false_negatives_dict.values()), axis=0)\n",
        "        for label in labels:\n",
        "            false_negatives_dict[label] = (np.array(false_negatives_dict[label]) / total_false_negatives_per_year) * 100\n",
        "\n",
        "        # Plot false negatives as a stacked bar plot\n",
        "        bottom = np.zeros(len(years))\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        for label in labels:\n",
        "            plt.bar(years, false_negatives_dict[label], bottom=bottom, label=f'Class {label}', color=land_use_colors[label])\n",
        "            bottom += np.array(false_negatives_dict[label])\n",
        "\n",
        "        plt.xlabel('Years')\n",
        "        plt.ylabel('Percentage of False Negatives')\n",
        "        plt.title('Percentage of False Negatives for Each Land-Use Class Across Years')\n",
        "    else:\n",
        "        # Plot total false negatives clustered by class\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        width = 0.1  # Width of each bar\n",
        "        x = np.arange(len(years))  # The label locations\n",
        "        for i, label in enumerate(labels):\n",
        "            plt.bar(x + i * width, false_negatives_dict[label], width, label=f'Class {label}', color=land_use_colors[label])\n",
        "\n",
        "        plt.xlabel('Years')\n",
        "        plt.ylabel('Total False Negatives')\n",
        "        plt.title('Total False Negatives for Each Land-Use Class Across Years')\n",
        "        plt.xticks(x + width * (len(labels) - 1) / 2, years, rotation=45)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the plot\n",
        "    false_neg_plot_path = os.path.join(save_folder, 'total_false_negatives_clustered.png' if not as_percentage else 'total_false_negatives_stacked.png')\n",
        "    plt.savefig(false_neg_plot_path)\n",
        "    plt.close()\n",
        "\n",
        "# Example usage\n",
        "folder_path = '/content/drive/MyDrive/AML_group_project/AML_project/datasets/final_project/validation/out/'  # Change to your folder path\n",
        "save_folder = '/content/drive/MyDrive/AML_group_project/AML_project/datasets/final_project/validation/out/figures'  # Change to your save folder path\n",
        "\n",
        "process_and_plot_confusion_matrices(folder_path, save_folder, as_percentage=True)\n"
      ],
      "metadata": {
        "id": "fvnsqaEOrXUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "class TextProcessor:\n",
        "    def __init__(self, folder_path, save_folder):\n",
        "        self.folder_path = folder_path\n",
        "        self.save_folder = save_folder\n",
        "        self.labels = ['1', '2', '3', '4', '5', '6', '7']\n",
        "        self.land_use_colors = {\n",
        "            '1': \"yellow\",\n",
        "            '2': \"limegreen\",\n",
        "            '3': \"forestgreen\",\n",
        "            '4': \"springgreen\",\n",
        "            '5': \"red\",\n",
        "            '6': \"lightgrey\",\n",
        "            '7': \"blue\"\n",
        "        }\n",
        "        os.makedirs(save_folder, exist_ok=True)\n",
        "\n",
        "    def read_confusion_matrix(self, file_path):\n",
        "        with open(file_path, 'r') as file:\n",
        "            lines = file.readlines()\n",
        "\n",
        "        matrix_start_idx = None\n",
        "        for idx, line in enumerate(lines):\n",
        "            if line.startswith(\"Confusion Matrix:\"):\n",
        "                matrix_start_idx = idx + 1\n",
        "                break\n",
        "\n",
        "        matrix = []\n",
        "        for line in lines[matrix_start_idx + 1:]:\n",
        "            parts = line.strip().split()[1:]  # Skip the row label\n",
        "            if all(part.isdigit() for part in parts):  # Ensure all parts are digits\n",
        "                matrix.append(list(map(int, parts)))\n",
        "\n",
        "        return np.array(matrix)\n",
        "\n",
        "    def plot_confusion_matrix(self, matrix, title, save_path):\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', xticklabels=self.labels, yticklabels=self.labels)\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.title(title)\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "\n",
        "    def calculate_false_negatives(self, matrix):\n",
        "        return matrix.sum(axis=1) - np.diag(matrix)\n",
        "\n",
        "    def process_files(self, as_percentage=False):\n",
        "        txt_files = sorted([f for f in os.listdir(self.folder_path) if f.endswith('.txt')], key=lambda x: int(x.split('.')[0]))\n",
        "\n",
        "        false_negatives_dict = {label: [] for label in self.labels}\n",
        "        years = []\n",
        "\n",
        "        for txt_file in txt_files:\n",
        "            file_path = os.path.join(self.folder_path, txt_file)\n",
        "            confusion_matrix = self.read_confusion_matrix(file_path)\n",
        "            if confusion_matrix.size > 0:\n",
        "                save_path = os.path.join(self.save_folder, f'{txt_file[:-4]}_confusion_matrix.png')\n",
        "                self.plot_confusion_matrix(confusion_matrix, title=f'Confusion Matrix: {txt_file[:-4]}', save_path=save_path)\n",
        "\n",
        "                false_negatives = self.calculate_false_negatives(confusion_matrix)\n",
        "                for i, label in enumerate(self.labels):\n",
        "                    false_negatives_dict[label].append(false_negatives[i])\n",
        "                years.append(txt_file[:-4])\n",
        "\n",
        "        self.plot_false_negatives(false_negatives_dict, years, as_percentage)\n",
        "\n",
        "    def plot_false_negatives(self, false_negatives_dict, years, as_percentage):\n",
        "        if as_percentage:\n",
        "            total_false_negatives_per_year = np.sum(list(false_negatives_dict.values()), axis=0)\n",
        "            for label in self.labels:\n",
        "                false_negatives_dict[label] = (np.array(false_negatives_dict[label]) / total_false_negatives_per_year) * 100\n",
        "\n",
        "            bottom = np.zeros(len(years))\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            for label in self.labels:\n",
        "                plt.bar(years, false_negatives_dict[label], bottom=bottom, label=f'Class {label}', color=self.land_use_colors[label])\n",
        "                bottom += np.array(false_negatives_dict[label])\n",
        "\n",
        "            plt.xlabel('Years')\n",
        "            plt.ylabel('Percentage of False Negatives')\n",
        "            plt.title('Percentage of False Negatives for Each Land-Use Class Across Years')\n",
        "        else:\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            width = 0.1\n",
        "            x = np.arange(len(years))\n",
        "            for i, label in enumerate(self.labels):\n",
        "                plt.bar(x + i * width, false_negatives_dict[label], width, label=f'Class {label}', color=self.land_use_colors[label])\n",
        "\n",
        "            plt.xlabel('Years')\n",
        "            plt.ylabel('Total False Negatives')\n",
        "            plt.title('Total False Negatives for Each Land-Use Class Across Years')\n",
        "            plt.xticks(x + width * (len(self.labels) - 1) / 2, years, rotation=45)\n",
        "\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        false_neg_plot_path = os.path.join(self.save_folder, 'total_false_negatives_clustered.png' if not as_percentage else 'total_false_negatives_stacked.png')\n",
        "        plt.savefig(false_neg_plot_path)\n",
        "        plt.close()\n",
        "\n",
        "    def read_classification_report(self, file_path):\n",
        "        with open(file_path, 'r') as file:\n",
        "            lines = file.readlines()\n",
        "\n",
        "        report_start_idx = None\n",
        "        for idx, line in enumerate(lines):\n",
        "            if line.strip().startswith(\"precision\"):\n",
        "                report_start_idx = idx\n",
        "                break\n",
        "\n",
        "        data = []\n",
        "        for line in lines[report_start_idx + 1:]:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) >= 5 and parts[0] in self.labels:  # Ensure there are enough parts and the first part is a label\n",
        "                data.append(parts[:5])  # Extract precision, recall, f1-score, support, and the label\n",
        "\n",
        "        return pd.DataFrame(data, columns=[\"class\", \"precision\", \"recall\", \"f1-score\", \"support\"]).astype({\"precision\": float, \"recall\": float, \"f1-score\": float, \"support\": float})\n",
        "\n",
        "    def process_classification_reports(self):\n",
        "        txt_files = sorted([f for f in os.listdir(self.folder_path) if f.endswith('.txt')], key=lambda x: int(x.split('.')[0]))\n",
        "        reports = []\n",
        "\n",
        "        for txt_file in txt_files:\n",
        "            file_path = os.path.join(self.folder_path, txt_file)\n",
        "            report = self.read_classification_report(file_path)\n",
        "            report[\"year\"] = txt_file[:-4]\n",
        "            reports.append(report)\n",
        "\n",
        "        combined_report = pd.concat(reports, ignore_index=True).drop_duplicates(subset=[\"class\", \"year\"])\n",
        "        combined_report_path = os.path.join(self.save_folder, 'combined_classification_report.csv')\n",
        "        combined_report.to_csv(combined_report_path, index=False)\n",
        "\n",
        "        combined_report_html_path = os.path.join(self.save_folder, 'combined_classification_report.html')\n",
        "        combined_report.to_html(combined_report_html_path, index=False)\n",
        "\n",
        "        return combined_report\n",
        "\n",
        "    def generate_yearly_report(self):\n",
        "        combined_report = pd.read_csv(os.path.join(self.save_folder, 'combined_classification_report.csv'))\n",
        "\n",
        "        years = combined_report['year'].unique()\n",
        "        for year in years:\n",
        "            yearly_report = combined_report[combined_report['year'] == year]\n",
        "            yearly_report_html_path = os.path.join(self.save_folder, f'classification_report_{year}.html')\n",
        "            yearly_report.to_html(yearly_report_html_path, index=False)\n",
        "\n",
        "# Example usage\n",
        "folder_path = '/content/drive/MyDrive/AML_group_project/AML_project/datasets/final_project/validation/out/'  # Change to your folder path\n",
        "save_folder = '/content/drive/MyDrive/AML_group_project/AML_project/datasets/final_project/validation/out/figures'  # Change to your save folder path\n",
        "\n",
        "processor = TextProcessor(folder_path, save_folder)\n",
        "processor.process_files(as_percentage=True)\n",
        "classification_report = processor.process_classification_reports()\n",
        "processor.generate_yearly_report()\n",
        "print(classification_report)"
      ],
      "metadata": {
        "id": "mgfiEhmLtQd8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "AML",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}